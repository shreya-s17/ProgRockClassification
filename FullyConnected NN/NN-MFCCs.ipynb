{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN-MFCCs.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"PWgtAnM3pGk0","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import librosa\n","\n","from torch.autograd import Variable\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K2lAHLKueK5s","colab_type":"text"},"cell_type":"markdown","source":["# Interface colab notebook with dataset in google drive"]},{"metadata":{"id":"4y_YdIwXroj4","colab_type":"code","outputId":"e7c57ae5-0346-4e7f-fa6e-2636a710cf05","executionInfo":{"status":"ok","timestamp":1554054192501,"user_tz":240,"elapsed":558,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"7vfrRFPhseQJ","colab_type":"code","outputId":"f3223f2a-9491-493d-a180-0bfc90370efc","executionInfo":{"status":"ok","timestamp":1554054655029,"user_tz":240,"elapsed":2102,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"cell_type":"code","source":["#Check google drive directory \n","!ls \"/content/drive/My Drive/Colab Notebooks/\""],"execution_count":28,"outputs":[{"output_type":"stream","text":[" NN-MFCCs.ipynb   NonProg  'Progressive Rock Songs'\n"],"name":"stdout"}]},{"metadata":{"id":"IMHAhkMletFF","colab_type":"text"},"cell_type":"markdown","source":["# Get mp3 file names"]},{"metadata":{"id":"n8a_zKaw3kym","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"cca27d4b-06a2-4a5c-ff7a-2466a935fbc4","executionInfo":{"status":"ok","timestamp":1554056363641,"user_tz":240,"elapsed":937,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}}},"cell_type":"code","source":["from google.colab import drive\n","\n","import glob\n","\n","filePaths_NonProg = glob.glob(\"/content/drive/My Drive/Colab Notebooks/NonProg/***.mp3\")\n","filePaths_Prog = glob.glob(\"/content/drive/My Drive/Colab Notebooks/Progressive Rock Songs/***.mp3\")\n","print(len(filePaths_NonProg))\n","print(len(filePaths_Prog))\n","numberNonProg = len(filePaths_NonProg)\n","numberProg = len(filePaths_Prog)\n","#for i in range(len(filePaths_Prog)):\n","#  print(filePaths_Prog[i])"],"execution_count":49,"outputs":[{"output_type":"stream","text":["302\n","73\n"],"name":"stdout"}]},{"metadata":{"id":"0ZREhRhueDOU","colab_type":"text"},"cell_type":"markdown","source":["# Extract Features - MFCCs and Covariance of MFCCs"]},{"metadata":{"id":"ua9BJmMm7iyz","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","numOfMFCCS = 20\n","\n","def getMFCCS(x, sr, nMFCCs):\n","    stft = np.abs(librosa.stft(x, n_fft=2048, hop_length=512))\n","    mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n","    mfccs = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=20)\n","    meanMFCCS = np.mean(mfccs, axis=1)\n","\n","    cov = np.cov(mfccs)\n","    upperCovIndicies = np.triu_indices(nMFCCs)\n","    upperCov = cov[upperCovIndicies]\n","    return meanMFCCS, upperCov, mfccs\n","\n","def normalizeInputs(x):\n","    mean = x.mean()\n","    std = x.std()\n","    z = (x-mean)/std\n","    return z"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GVadxlKQe8Y_","colab_type":"text"},"cell_type":"markdown","source":["Grab features from files, store into matrix"]},{"metadata":{"id":"GWLHqiyI2cHV","colab_type":"code","colab":{}},"cell_type":"code","source":["nonProgInputs = np.zeros((numberNonProg, inputSize))\n","for i, file in enumerate(filePaths_NonProg):\n","  print(i)\n","  x, sr = librosa.load(file, sr=None, mono=True, duration = 40)\n","  meanMFCC, covMFCC, mfccs = getMFCCS(x, sr, numOfMFCCS)\n","  meanMFCCNormalized = normalizeInputs(meanMFCC)\n","  covMFCCNormalized = normalizeInputs(covMFCC)\n","  \n","  inputs = np.concatenate((meanMFCCNormalized, covMFCCNormalized))\n","  inputs = np.expand_dims(inputs,axis=1).T\n","\n","  nonProgInputs[i,:] = inputs\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"HzvgJtKM-_GD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1258},"outputId":"1ded950e-123f-47ad-df68-a46482358548","executionInfo":{"status":"ok","timestamp":1554057668811,"user_tz":240,"elapsed":85088,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}}},"cell_type":"code","source":["progInputs = np.zeros((numberProg, inputSize))\n","for i, file in enumerate(filePaths_Prog):\n","  print(i)\n","  x, sr = librosa.load(file, sr=None, mono=True, duration = 40)\n","  meanMFCC, covMFCC, mfccs = getMFCCS(x, sr, numOfMFCCS)\n","  meanMFCCNormalized = normalizeInputs(meanMFCC)\n","  covMFCCNormalized = normalizeInputs(covMFCC)\n","  \n","  inputs = np.concatenate((meanMFCCNormalized, covMFCCNormalized))\n","  inputs = np.expand_dims(inputs,axis=1).T\n","\n","  progInputs[i,:] = inputs\n","\n","\n"],"execution_count":60,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n"],"name":"stdout"}]},{"metadata":{"id":"BA-FcJwH3iGc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"26ef9616-8e58-42e0-afa1-3f00852222a9","executionInfo":{"status":"ok","timestamp":1554057785135,"user_tz":240,"elapsed":312,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}}},"cell_type":"code","source":["#Combine prog with non prog inputs\n","allInputs = np.concatenate((progInputs, nonProgInputs))\n","allInputs.shape"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(375, 230)"]},"metadata":{"tags":[]},"execution_count":63}]},{"metadata":{"id":"jv3C0mYIfLnN","colab_type":"text"},"cell_type":"markdown","source":["# Create data labels"]},{"metadata":{"id":"5NTwr8pdEowe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"afb8d7d9-a0fe-44d0-e183-8f3cfe415475","executionInfo":{"status":"ok","timestamp":1554058210526,"user_tz":240,"elapsed":303,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}}},"cell_type":"code","source":["#Create labels \n","yProg = np.ones((numberProg,1))\n","yNonProg = -np.ones((numberNonProg,1))\n","y = np.concatenate((yProg, yNonProg))"],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(375, 1)"]},"metadata":{"tags":[]},"execution_count":79}]},{"metadata":{"id":"bQPsh-WTfZUd","colab_type":"text"},"cell_type":"markdown","source":["# Split dataset into training set and test set"]},{"metadata":{"id":"yPGSXvhhF5Ed","colab_type":"code","colab":{}},"cell_type":"code","source":["#splitting the training set\n","testSize = .2\n","randStateSplit = 42\n","inputTrain, inputTest, labelTrain, labelTest = train_test_split(allInputs, y, test_size=testSize, random_state = randStateSplit, shuffle = True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xeAw4j18fjWm","colab_type":"text"},"cell_type":"markdown","source":["# Convert our dataset into a pseudo-pytorch data loader format to feed into the Neural Network (can be improved)"]},{"metadata":{"id":"XV6RNjB-NMKa","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","numTrainingSamples = inputTrain.shape[0]\n","input_size = inputTrain.shape[1]\n","\n","batch_size = 25\n","numOfTrainingBatches = 12\n","\n","inputTrainPD = pd.DataFrame(np.concatenate((labelTrain, inputTrain),axis = 1))\n","\n","train_loader = []\n","labels = []\n","for g, temp_df in inputTrainPD.groupby(np.arange(numTrainingSamples) // batch_size):\n","  tempLabels = np.asarray(temp_df.iloc[:,0])\n","  tempLabels = torch.LongTensor(tempLabels)\n","  tempLabelsSqueezed = torch.squeeze(tempLabels)\n","  labels.append(tempLabelsSqueezed)\n","\n","  tempInputs = temp_df.iloc[:,1:].as_matrix() #convert to np array ... as_matrix returns np array\n","  torchSamples = []\n","  for i in range(batch_size):\n","    #import pdb; pdb.set_trace()\n","    tempTorchSample = torch.FloatTensor(tempInputs[i])\n","    torchSamples.append(torch.t(tempTorchSample.view(input_size,1)))\n","  #   import pdb; pdb.set_trace()\n","  stackedInputTensors = torch.stack(torchSamples)    \n","\n","  train_loader.append(stackedInputTensors.view(batch_size,input_size))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KEjq4R9rfuRx","colab_type":"text"},"cell_type":"markdown","source":["# Neural Network Architecture"]},{"metadata":{"id":"bGR6z-25fxxV","colab_type":"code","colab":{}},"cell_type":"code","source":["inputSize = 230\n","hidden_size1 = 200\n","hidden_size2 = 50\n","num_classes = 1\n","# Fully connected neural network with one hidden layer\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size1) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size1, hidden_size2) \n","        self.relu = nn.ReLU()\n","        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        out = self.relu(out)\n","        out = self.fc3(out)\n","        return out\n","\n","net = NeuralNet(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rcZK3Ee1gCmY","colab_type":"text"},"cell_type":"markdown","source":["# Train model"]},{"metadata":{"id":"QJrmU7ADIf7C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":20417},"outputId":"f78fde23-9506-47ce-b91b-4576046d2a55","executionInfo":{"status":"ok","timestamp":1554065323227,"user_tz":240,"elapsed":1671,"user":{"displayName":"Nick Kroeger","photoUrl":"","userId":"17654142378335071446"}}},"cell_type":"code","source":["num_epochs = 100\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i in range(numOfTrainingBatches):  \n","        # Move tensors to the configured device\n","        input_x = Variable(train_loader[i])\n","        label_x = Variable(labels[i])\n","        \n","        # Forward pass\n","        outputs = net(input_x.float())\n","        outputs = torch.squeeze(outputs)\n","        \n","        #import pdb; pdb.set_trace()\n","        loss = criterion(outputs, label_x.float())\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        #if (i+1) % 100 == 0:\n","        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"],"execution_count":137,"outputs":[{"output_type":"stream","text":["Epoch [1/100], Step [1/12], Loss: 0.8378\n","Epoch [1/100], Step [2/12], Loss: 0.7810\n","Epoch [1/100], Step [3/12], Loss: 0.7442\n","Epoch [1/100], Step [4/12], Loss: 0.8049\n","Epoch [1/100], Step [5/12], Loss: 0.6984\n","Epoch [1/100], Step [6/12], Loss: 0.7404\n","Epoch [1/100], Step [7/12], Loss: 0.8758\n","Epoch [1/100], Step [8/12], Loss: 0.8065\n","Epoch [1/100], Step [9/12], Loss: 0.8285\n","Epoch [1/100], Step [10/12], Loss: 0.8316\n","Epoch [1/100], Step [11/12], Loss: 0.8194\n","Epoch [1/100], Step [12/12], Loss: 0.7107\n","Epoch [2/100], Step [1/12], Loss: 0.8378\n","Epoch [2/100], Step [2/12], Loss: 0.7810\n","Epoch [2/100], Step [3/12], Loss: 0.7442\n","Epoch [2/100], Step [4/12], Loss: 0.8049\n","Epoch [2/100], Step [5/12], Loss: 0.6984\n","Epoch [2/100], Step [6/12], Loss: 0.7404\n","Epoch [2/100], Step [7/12], Loss: 0.8758\n","Epoch [2/100], Step [8/12], Loss: 0.8065\n","Epoch [2/100], Step [9/12], Loss: 0.8285\n","Epoch [2/100], Step [10/12], Loss: 0.8316\n","Epoch [2/100], Step [11/12], Loss: 0.8194\n","Epoch [2/100], Step [12/12], Loss: 0.7107\n","Epoch [3/100], Step [1/12], Loss: 0.8378\n","Epoch [3/100], Step [2/12], Loss: 0.7810\n","Epoch [3/100], Step [3/12], Loss: 0.7442\n","Epoch [3/100], Step [4/12], Loss: 0.8049\n","Epoch [3/100], Step [5/12], Loss: 0.6984\n","Epoch [3/100], Step [6/12], Loss: 0.7404\n","Epoch [3/100], Step [7/12], Loss: 0.8758\n","Epoch [3/100], Step [8/12], Loss: 0.8065\n","Epoch [3/100], Step [9/12], Loss: 0.8285\n","Epoch [3/100], Step [10/12], Loss: 0.8316\n","Epoch [3/100], Step [11/12], Loss: 0.8194\n","Epoch [3/100], Step [12/12], Loss: 0.7107\n","Epoch [4/100], Step [1/12], Loss: 0.8378\n","Epoch [4/100], Step [2/12], Loss: 0.7810\n","Epoch [4/100], Step [3/12], Loss: 0.7442\n","Epoch [4/100], Step [4/12], Loss: 0.8049\n","Epoch [4/100], Step [5/12], Loss: 0.6984\n","Epoch [4/100], Step [6/12], Loss: 0.7404\n","Epoch [4/100], Step [7/12], Loss: 0.8758\n","Epoch [4/100], Step [8/12], Loss: 0.8065\n","Epoch [4/100], Step [9/12], Loss: 0.8285\n","Epoch [4/100], Step [10/12], Loss: 0.8316\n","Epoch [4/100], Step [11/12], Loss: 0.8194\n","Epoch [4/100], Step [12/12], Loss: 0.7107\n","Epoch [5/100], Step [1/12], Loss: 0.8378\n","Epoch [5/100], Step [2/12], Loss: 0.7810\n","Epoch [5/100], Step [3/12], Loss: 0.7442\n","Epoch [5/100], Step [4/12], Loss: 0.8049\n","Epoch [5/100], Step [5/12], Loss: 0.6984\n","Epoch [5/100], Step [6/12], Loss: 0.7404\n","Epoch [5/100], Step [7/12], Loss: 0.8758\n","Epoch [5/100], Step [8/12], Loss: 0.8065\n","Epoch [5/100], Step [9/12], Loss: 0.8285\n","Epoch [5/100], Step [10/12], Loss: 0.8316\n","Epoch [5/100], Step [11/12], Loss: 0.8194\n","Epoch [5/100], Step [12/12], Loss: 0.7107\n","Epoch [6/100], Step [1/12], Loss: 0.8378\n","Epoch [6/100], Step [2/12], Loss: 0.7810\n","Epoch [6/100], Step [3/12], Loss: 0.7442\n","Epoch [6/100], Step [4/12], Loss: 0.8049\n","Epoch [6/100], Step [5/12], Loss: 0.6984\n","Epoch [6/100], Step [6/12], Loss: 0.7404\n","Epoch [6/100], Step [7/12], Loss: 0.8758\n","Epoch [6/100], Step [8/12], Loss: 0.8065\n","Epoch [6/100], Step [9/12], Loss: 0.8285\n","Epoch [6/100], Step [10/12], Loss: 0.8316\n","Epoch [6/100], Step [11/12], Loss: 0.8194\n","Epoch [6/100], Step [12/12], Loss: 0.7107\n","Epoch [7/100], Step [1/12], Loss: 0.8378\n","Epoch [7/100], Step [2/12], Loss: 0.7810\n","Epoch [7/100], Step [3/12], Loss: 0.7442\n","Epoch [7/100], Step [4/12], Loss: 0.8049\n","Epoch [7/100], Step [5/12], Loss: 0.6984\n","Epoch [7/100], Step [6/12], Loss: 0.7404\n","Epoch [7/100], Step [7/12], Loss: 0.8758\n","Epoch [7/100], Step [8/12], Loss: 0.8065\n","Epoch [7/100], Step [9/12], Loss: 0.8285\n","Epoch [7/100], Step [10/12], Loss: 0.8316\n","Epoch [7/100], Step [11/12], Loss: 0.8194\n","Epoch [7/100], Step [12/12], Loss: 0.7107\n","Epoch [8/100], Step [1/12], Loss: 0.8378\n","Epoch [8/100], Step [2/12], Loss: 0.7810\n","Epoch [8/100], Step [3/12], Loss: 0.7442\n","Epoch [8/100], Step [4/12], Loss: 0.8049\n","Epoch [8/100], Step [5/12], Loss: 0.6984\n","Epoch [8/100], Step [6/12], Loss: 0.7404\n","Epoch [8/100], Step [7/12], Loss: 0.8758\n","Epoch [8/100], Step [8/12], Loss: 0.8065\n","Epoch [8/100], Step [9/12], Loss: 0.8285\n","Epoch [8/100], Step [10/12], Loss: 0.8316\n","Epoch [8/100], Step [11/12], Loss: 0.8194\n","Epoch [8/100], Step [12/12], Loss: 0.7107\n","Epoch [9/100], Step [1/12], Loss: 0.8378\n","Epoch [9/100], Step [2/12], Loss: 0.7810\n","Epoch [9/100], Step [3/12], Loss: 0.7442\n","Epoch [9/100], Step [4/12], Loss: 0.8049\n","Epoch [9/100], Step [5/12], Loss: 0.6984\n","Epoch [9/100], Step [6/12], Loss: 0.7404\n","Epoch [9/100], Step [7/12], Loss: 0.8758\n","Epoch [9/100], Step [8/12], Loss: 0.8065\n","Epoch [9/100], Step [9/12], Loss: 0.8285\n","Epoch [9/100], Step [10/12], Loss: 0.8316\n","Epoch [9/100], Step [11/12], Loss: 0.8194\n","Epoch [9/100], Step [12/12], Loss: 0.7107\n","Epoch [10/100], Step [1/12], Loss: 0.8378\n","Epoch [10/100], Step [2/12], Loss: 0.7810\n","Epoch [10/100], Step [3/12], Loss: 0.7442\n","Epoch [10/100], Step [4/12], Loss: 0.8049\n","Epoch [10/100], Step [5/12], Loss: 0.6984\n","Epoch [10/100], Step [6/12], Loss: 0.7404\n","Epoch [10/100], Step [7/12], Loss: 0.8758\n","Epoch [10/100], Step [8/12], Loss: 0.8065\n","Epoch [10/100], Step [9/12], Loss: 0.8285\n","Epoch [10/100], Step [10/12], Loss: 0.8316\n","Epoch [10/100], Step [11/12], Loss: 0.8194\n","Epoch [10/100], Step [12/12], Loss: 0.7107\n","Epoch [11/100], Step [1/12], Loss: 0.8378\n","Epoch [11/100], Step [2/12], Loss: 0.7810\n","Epoch [11/100], Step [3/12], Loss: 0.7442\n","Epoch [11/100], Step [4/12], Loss: 0.8049\n","Epoch [11/100], Step [5/12], Loss: 0.6984\n","Epoch [11/100], Step [6/12], Loss: 0.7404\n","Epoch [11/100], Step [7/12], Loss: 0.8758\n","Epoch [11/100], Step [8/12], Loss: 0.8065\n","Epoch [11/100], Step [9/12], Loss: 0.8285\n","Epoch [11/100], Step [10/12], Loss: 0.8316\n","Epoch [11/100], Step [11/12], Loss: 0.8194\n","Epoch [11/100], Step [12/12], Loss: 0.7107\n","Epoch [12/100], Step [1/12], Loss: 0.8378\n","Epoch [12/100], Step [2/12], Loss: 0.7810\n","Epoch [12/100], Step [3/12], Loss: 0.7442\n","Epoch [12/100], Step [4/12], Loss: 0.8049\n","Epoch [12/100], Step [5/12], Loss: 0.6984\n","Epoch [12/100], Step [6/12], Loss: 0.7404\n","Epoch [12/100], Step [7/12], Loss: 0.8758\n","Epoch [12/100], Step [8/12], Loss: 0.8065\n","Epoch [12/100], Step [9/12], Loss: 0.8285\n","Epoch [12/100], Step [10/12], Loss: 0.8316\n","Epoch [12/100], Step [11/12], Loss: 0.8194\n","Epoch [12/100], Step [12/12], Loss: 0.7107\n","Epoch [13/100], Step [1/12], Loss: 0.8378\n","Epoch [13/100], Step [2/12], Loss: 0.7810\n","Epoch [13/100], Step [3/12], Loss: 0.7442\n","Epoch [13/100], Step [4/12], Loss: 0.8049\n","Epoch [13/100], Step [5/12], Loss: 0.6984\n","Epoch [13/100], Step [6/12], Loss: 0.7404\n","Epoch [13/100], Step [7/12], Loss: 0.8758\n","Epoch [13/100], Step [8/12], Loss: 0.8065\n","Epoch [13/100], Step [9/12], Loss: 0.8285\n","Epoch [13/100], Step [10/12], Loss: 0.8316\n","Epoch [13/100], Step [11/12], Loss: 0.8194\n","Epoch [13/100], Step [12/12], Loss: 0.7107\n","Epoch [14/100], Step [1/12], Loss: 0.8378\n","Epoch [14/100], Step [2/12], Loss: 0.7810\n","Epoch [14/100], Step [3/12], Loss: 0.7442\n","Epoch [14/100], Step [4/12], Loss: 0.8049\n","Epoch [14/100], Step [5/12], Loss: 0.6984\n","Epoch [14/100], Step [6/12], Loss: 0.7404\n","Epoch [14/100], Step [7/12], Loss: 0.8758\n","Epoch [14/100], Step [8/12], Loss: 0.8065\n","Epoch [14/100], Step [9/12], Loss: 0.8285\n","Epoch [14/100], Step [10/12], Loss: 0.8316\n","Epoch [14/100], Step [11/12], Loss: 0.8194\n","Epoch [14/100], Step [12/12], Loss: 0.7107\n","Epoch [15/100], Step [1/12], Loss: 0.8378\n","Epoch [15/100], Step [2/12], Loss: 0.7810\n","Epoch [15/100], Step [3/12], Loss: 0.7442\n","Epoch [15/100], Step [4/12], Loss: 0.8049\n","Epoch [15/100], Step [5/12], Loss: 0.6984\n","Epoch [15/100], Step [6/12], Loss: 0.7404\n","Epoch [15/100], Step [7/12], Loss: 0.8758\n","Epoch [15/100], Step [8/12], Loss: 0.8065\n","Epoch [15/100], Step [9/12], Loss: 0.8285\n","Epoch [15/100], Step [10/12], Loss: 0.8316\n","Epoch [15/100], Step [11/12], Loss: 0.8194\n","Epoch [15/100], Step [12/12], Loss: 0.7107\n","Epoch [16/100], Step [1/12], Loss: 0.8378\n","Epoch [16/100], Step [2/12], Loss: 0.7810\n","Epoch [16/100], Step [3/12], Loss: 0.7442\n","Epoch [16/100], Step [4/12], Loss: 0.8049\n","Epoch [16/100], Step [5/12], Loss: 0.6984\n","Epoch [16/100], Step [6/12], Loss: 0.7404\n","Epoch [16/100], Step [7/12], Loss: 0.8758\n","Epoch [16/100], Step [8/12], Loss: 0.8065\n","Epoch [16/100], Step [9/12], Loss: 0.8285\n","Epoch [16/100], Step [10/12], Loss: 0.8316\n","Epoch [16/100], Step [11/12], Loss: 0.8194\n","Epoch [16/100], Step [12/12], Loss: 0.7107\n","Epoch [17/100], Step [1/12], Loss: 0.8378\n","Epoch [17/100], Step [2/12], Loss: 0.7810\n","Epoch [17/100], Step [3/12], Loss: 0.7442\n","Epoch [17/100], Step [4/12], Loss: 0.8049\n","Epoch [17/100], Step [5/12], Loss: 0.6984\n","Epoch [17/100], Step [6/12], Loss: 0.7404\n","Epoch [17/100], Step [7/12], Loss: 0.8758\n","Epoch [17/100], Step [8/12], Loss: 0.8065\n","Epoch [17/100], Step [9/12], Loss: 0.8285\n","Epoch [17/100], Step [10/12], Loss: 0.8316\n","Epoch [17/100], Step [11/12], Loss: 0.8194\n","Epoch [17/100], Step [12/12], Loss: 0.7107\n","Epoch [18/100], Step [1/12], Loss: 0.8378\n","Epoch [18/100], Step [2/12], Loss: 0.7810\n","Epoch [18/100], Step [3/12], Loss: 0.7442\n","Epoch [18/100], Step [4/12], Loss: 0.8049\n","Epoch [18/100], Step [5/12], Loss: 0.6984\n","Epoch [18/100], Step [6/12], Loss: 0.7404\n","Epoch [18/100], Step [7/12], Loss: 0.8758\n","Epoch [18/100], Step [8/12], Loss: 0.8065\n","Epoch [18/100], Step [9/12], Loss: 0.8285\n","Epoch [18/100], Step [10/12], Loss: 0.8316\n","Epoch [18/100], Step [11/12], Loss: 0.8194\n","Epoch [18/100], Step [12/12], Loss: 0.7107\n","Epoch [19/100], Step [1/12], Loss: 0.8378\n","Epoch [19/100], Step [2/12], Loss: 0.7810\n","Epoch [19/100], Step [3/12], Loss: 0.7442\n","Epoch [19/100], Step [4/12], Loss: 0.8049\n","Epoch [19/100], Step [5/12], Loss: 0.6984\n","Epoch [19/100], Step [6/12], Loss: 0.7404\n","Epoch [19/100], Step [7/12], Loss: 0.8758\n","Epoch [19/100], Step [8/12], Loss: 0.8065\n","Epoch [19/100], Step [9/12], Loss: 0.8285\n","Epoch [19/100], Step [10/12], Loss: 0.8316\n","Epoch [19/100], Step [11/12], Loss: 0.8194\n","Epoch [19/100], Step [12/12], Loss: 0.7107\n","Epoch [20/100], Step [1/12], Loss: 0.8378\n","Epoch [20/100], Step [2/12], Loss: 0.7810\n","Epoch [20/100], Step [3/12], Loss: 0.7442\n","Epoch [20/100], Step [4/12], Loss: 0.8049\n","Epoch [20/100], Step [5/12], Loss: 0.6984\n","Epoch [20/100], Step [6/12], Loss: 0.7404\n","Epoch [20/100], Step [7/12], Loss: 0.8758\n","Epoch [20/100], Step [8/12], Loss: 0.8065\n","Epoch [20/100], Step [9/12], Loss: 0.8285\n","Epoch [20/100], Step [10/12], Loss: 0.8316\n","Epoch [20/100], Step [11/12], Loss: 0.8194\n","Epoch [20/100], Step [12/12], Loss: 0.7107\n","Epoch [21/100], Step [1/12], Loss: 0.8378\n","Epoch [21/100], Step [2/12], Loss: 0.7810\n","Epoch [21/100], Step [3/12], Loss: 0.7442\n","Epoch [21/100], Step [4/12], Loss: 0.8049\n","Epoch [21/100], Step [5/12], Loss: 0.6984\n","Epoch [21/100], Step [6/12], Loss: 0.7404\n","Epoch [21/100], Step [7/12], Loss: 0.8758\n","Epoch [21/100], Step [8/12], Loss: 0.8065\n","Epoch [21/100], Step [9/12], Loss: 0.8285\n","Epoch [21/100], Step [10/12], Loss: 0.8316\n","Epoch [21/100], Step [11/12], Loss: 0.8194\n","Epoch [21/100], Step [12/12], Loss: 0.7107\n","Epoch [22/100], Step [1/12], Loss: 0.8378\n","Epoch [22/100], Step [2/12], Loss: 0.7810\n","Epoch [22/100], Step [3/12], Loss: 0.7442\n","Epoch [22/100], Step [4/12], Loss: 0.8049\n","Epoch [22/100], Step [5/12], Loss: 0.6984\n","Epoch [22/100], Step [6/12], Loss: 0.7404\n","Epoch [22/100], Step [7/12], Loss: 0.8758\n","Epoch [22/100], Step [8/12], Loss: 0.8065\n","Epoch [22/100], Step [9/12], Loss: 0.8285\n","Epoch [22/100], Step [10/12], Loss: 0.8316\n","Epoch [22/100], Step [11/12], Loss: 0.8194\n","Epoch [22/100], Step [12/12], Loss: 0.7107\n","Epoch [23/100], Step [1/12], Loss: 0.8378\n","Epoch [23/100], Step [2/12], Loss: 0.7810\n","Epoch [23/100], Step [3/12], Loss: 0.7442\n","Epoch [23/100], Step [4/12], Loss: 0.8049\n","Epoch [23/100], Step [5/12], Loss: 0.6984\n","Epoch [23/100], Step [6/12], Loss: 0.7404\n","Epoch [23/100], Step [7/12], Loss: 0.8758\n","Epoch [23/100], Step [8/12], Loss: 0.8065\n","Epoch [23/100], Step [9/12], Loss: 0.8285\n","Epoch [23/100], Step [10/12], Loss: 0.8316\n","Epoch [23/100], Step [11/12], Loss: 0.8194\n","Epoch [23/100], Step [12/12], Loss: 0.7107\n","Epoch [24/100], Step [1/12], Loss: 0.8378\n","Epoch [24/100], Step [2/12], Loss: 0.7810\n","Epoch [24/100], Step [3/12], Loss: 0.7442\n","Epoch [24/100], Step [4/12], Loss: 0.8049\n","Epoch [24/100], Step [5/12], Loss: 0.6984\n","Epoch [24/100], Step [6/12], Loss: 0.7404\n","Epoch [24/100], Step [7/12], Loss: 0.8758\n","Epoch [24/100], Step [8/12], Loss: 0.8065\n","Epoch [24/100], Step [9/12], Loss: 0.8285\n","Epoch [24/100], Step [10/12], Loss: 0.8316\n","Epoch [24/100], Step [11/12], Loss: 0.8194\n","Epoch [24/100], Step [12/12], Loss: 0.7107\n","Epoch [25/100], Step [1/12], Loss: 0.8378\n","Epoch [25/100], Step [2/12], Loss: 0.7810\n","Epoch [25/100], Step [3/12], Loss: 0.7442\n","Epoch [25/100], Step [4/12], Loss: 0.8049\n","Epoch [25/100], Step [5/12], Loss: 0.6984\n","Epoch [25/100], Step [6/12], Loss: 0.7404\n","Epoch [25/100], Step [7/12], Loss: 0.8758\n","Epoch [25/100], Step [8/12], Loss: 0.8065\n","Epoch [25/100], Step [9/12], Loss: 0.8285\n","Epoch [25/100], Step [10/12], Loss: 0.8316\n","Epoch [25/100], Step [11/12], Loss: 0.8194\n","Epoch [25/100], Step [12/12], Loss: 0.7107\n","Epoch [26/100], Step [1/12], Loss: 0.8378\n","Epoch [26/100], Step [2/12], Loss: 0.7810\n","Epoch [26/100], Step [3/12], Loss: 0.7442\n","Epoch [26/100], Step [4/12], Loss: 0.8049\n","Epoch [26/100], Step [5/12], Loss: 0.6984\n","Epoch [26/100], Step [6/12], Loss: 0.7404\n","Epoch [26/100], Step [7/12], Loss: 0.8758\n","Epoch [26/100], Step [8/12], Loss: 0.8065\n","Epoch [26/100], Step [9/12], Loss: 0.8285\n","Epoch [26/100], Step [10/12], Loss: 0.8316\n","Epoch [26/100], Step [11/12], Loss: 0.8194\n","Epoch [26/100], Step [12/12], Loss: 0.7107\n","Epoch [27/100], Step [1/12], Loss: 0.8378\n","Epoch [27/100], Step [2/12], Loss: 0.7810\n","Epoch [27/100], Step [3/12], Loss: 0.7442\n","Epoch [27/100], Step [4/12], Loss: 0.8049\n","Epoch [27/100], Step [5/12], Loss: 0.6984\n","Epoch [27/100], Step [6/12], Loss: 0.7404\n","Epoch [27/100], Step [7/12], Loss: 0.8758\n","Epoch [27/100], Step [8/12], Loss: 0.8065\n","Epoch [27/100], Step [9/12], Loss: 0.8285\n","Epoch [27/100], Step [10/12], Loss: 0.8316\n","Epoch [27/100], Step [11/12], Loss: 0.8194\n","Epoch [27/100], Step [12/12], Loss: 0.7107\n","Epoch [28/100], Step [1/12], Loss: 0.8378\n","Epoch [28/100], Step [2/12], Loss: 0.7810\n","Epoch [28/100], Step [3/12], Loss: 0.7442\n","Epoch [28/100], Step [4/12], Loss: 0.8049\n","Epoch [28/100], Step [5/12], Loss: 0.6984\n","Epoch [28/100], Step [6/12], Loss: 0.7404\n","Epoch [28/100], Step [7/12], Loss: 0.8758\n","Epoch [28/100], Step [8/12], Loss: 0.8065\n","Epoch [28/100], Step [9/12], Loss: 0.8285\n","Epoch [28/100], Step [10/12], Loss: 0.8316\n","Epoch [28/100], Step [11/12], Loss: 0.8194\n","Epoch [28/100], Step [12/12], Loss: 0.7107\n","Epoch [29/100], Step [1/12], Loss: 0.8378\n","Epoch [29/100], Step [2/12], Loss: 0.7810\n","Epoch [29/100], Step [3/12], Loss: 0.7442\n","Epoch [29/100], Step [4/12], Loss: 0.8049\n","Epoch [29/100], Step [5/12], Loss: 0.6984\n","Epoch [29/100], Step [6/12], Loss: 0.7404\n","Epoch [29/100], Step [7/12], Loss: 0.8758\n","Epoch [29/100], Step [8/12], Loss: 0.8065\n","Epoch [29/100], Step [9/12], Loss: 0.8285\n","Epoch [29/100], Step [10/12], Loss: 0.8316\n","Epoch [29/100], Step [11/12], Loss: 0.8194\n","Epoch [29/100], Step [12/12], Loss: 0.7107\n","Epoch [30/100], Step [1/12], Loss: 0.8378\n","Epoch [30/100], Step [2/12], Loss: 0.7810\n","Epoch [30/100], Step [3/12], Loss: 0.7442\n","Epoch [30/100], Step [4/12], Loss: 0.8049\n","Epoch [30/100], Step [5/12], Loss: 0.6984\n","Epoch [30/100], Step [6/12], Loss: 0.7404\n","Epoch [30/100], Step [7/12], Loss: 0.8758\n","Epoch [30/100], Step [8/12], Loss: 0.8065\n","Epoch [30/100], Step [9/12], Loss: 0.8285\n","Epoch [30/100], Step [10/12], Loss: 0.8316\n","Epoch [30/100], Step [11/12], Loss: 0.8194\n","Epoch [30/100], Step [12/12], Loss: 0.7107\n","Epoch [31/100], Step [1/12], Loss: 0.8378\n","Epoch [31/100], Step [2/12], Loss: 0.7810\n","Epoch [31/100], Step [3/12], Loss: 0.7442\n","Epoch [31/100], Step [4/12], Loss: 0.8049\n","Epoch [31/100], Step [5/12], Loss: 0.6984\n","Epoch [31/100], Step [6/12], Loss: 0.7404\n","Epoch [31/100], Step [7/12], Loss: 0.8758\n","Epoch [31/100], Step [8/12], Loss: 0.8065\n","Epoch [31/100], Step [9/12], Loss: 0.8285\n","Epoch [31/100], Step [10/12], Loss: 0.8316\n","Epoch [31/100], Step [11/12], Loss: 0.8194\n","Epoch [31/100], Step [12/12], Loss: 0.7107\n","Epoch [32/100], Step [1/12], Loss: 0.8378\n","Epoch [32/100], Step [2/12], Loss: 0.7810\n","Epoch [32/100], Step [3/12], Loss: 0.7442\n","Epoch [32/100], Step [4/12], Loss: 0.8049\n","Epoch [32/100], Step [5/12], Loss: 0.6984\n","Epoch [32/100], Step [6/12], Loss: 0.7404\n","Epoch [32/100], Step [7/12], Loss: 0.8758\n","Epoch [32/100], Step [8/12], Loss: 0.8065\n","Epoch [32/100], Step [9/12], Loss: 0.8285\n","Epoch [32/100], Step [10/12], Loss: 0.8316\n","Epoch [32/100], Step [11/12], Loss: 0.8194\n","Epoch [32/100], Step [12/12], Loss: 0.7107\n","Epoch [33/100], Step [1/12], Loss: 0.8378\n","Epoch [33/100], Step [2/12], Loss: 0.7810\n","Epoch [33/100], Step [3/12], Loss: 0.7442\n","Epoch [33/100], Step [4/12], Loss: 0.8049\n","Epoch [33/100], Step [5/12], Loss: 0.6984\n","Epoch [33/100], Step [6/12], Loss: 0.7404\n","Epoch [33/100], Step [7/12], Loss: 0.8758\n","Epoch [33/100], Step [8/12], Loss: 0.8065\n","Epoch [33/100], Step [9/12], Loss: 0.8285\n","Epoch [33/100], Step [10/12], Loss: 0.8316\n","Epoch [33/100], Step [11/12], Loss: 0.8194\n","Epoch [33/100], Step [12/12], Loss: 0.7107\n","Epoch [34/100], Step [1/12], Loss: 0.8378\n","Epoch [34/100], Step [2/12], Loss: 0.7810\n","Epoch [34/100], Step [3/12], Loss: 0.7442\n","Epoch [34/100], Step [4/12], Loss: 0.8049\n","Epoch [34/100], Step [5/12], Loss: 0.6984\n","Epoch [34/100], Step [6/12], Loss: 0.7404\n","Epoch [34/100], Step [7/12], Loss: 0.8758\n","Epoch [34/100], Step [8/12], Loss: 0.8065\n","Epoch [34/100], Step [9/12], Loss: 0.8285\n","Epoch [34/100], Step [10/12], Loss: 0.8316\n","Epoch [34/100], Step [11/12], Loss: 0.8194\n","Epoch [34/100], Step [12/12], Loss: 0.7107\n","Epoch [35/100], Step [1/12], Loss: 0.8378\n","Epoch [35/100], Step [2/12], Loss: 0.7810\n","Epoch [35/100], Step [3/12], Loss: 0.7442\n","Epoch [35/100], Step [4/12], Loss: 0.8049\n","Epoch [35/100], Step [5/12], Loss: 0.6984\n","Epoch [35/100], Step [6/12], Loss: 0.7404\n","Epoch [35/100], Step [7/12], Loss: 0.8758\n","Epoch [35/100], Step [8/12], Loss: 0.8065\n","Epoch [35/100], Step [9/12], Loss: 0.8285\n","Epoch [35/100], Step [10/12], Loss: 0.8316\n","Epoch [35/100], Step [11/12], Loss: 0.8194\n","Epoch [35/100], Step [12/12], Loss: 0.7107\n","Epoch [36/100], Step [1/12], Loss: 0.8378\n","Epoch [36/100], Step [2/12], Loss: 0.7810\n","Epoch [36/100], Step [3/12], Loss: 0.7442\n","Epoch [36/100], Step [4/12], Loss: 0.8049\n","Epoch [36/100], Step [5/12], Loss: 0.6984\n","Epoch [36/100], Step [6/12], Loss: 0.7404\n","Epoch [36/100], Step [7/12], Loss: 0.8758\n","Epoch [36/100], Step [8/12], Loss: 0.8065\n","Epoch [36/100], Step [9/12], Loss: 0.8285\n","Epoch [36/100], Step [10/12], Loss: 0.8316\n","Epoch [36/100], Step [11/12], Loss: 0.8194\n","Epoch [36/100], Step [12/12], Loss: 0.7107\n","Epoch [37/100], Step [1/12], Loss: 0.8378\n","Epoch [37/100], Step [2/12], Loss: 0.7810\n","Epoch [37/100], Step [3/12], Loss: 0.7442\n","Epoch [37/100], Step [4/12], Loss: 0.8049\n","Epoch [37/100], Step [5/12], Loss: 0.6984\n","Epoch [37/100], Step [6/12], Loss: 0.7404\n","Epoch [37/100], Step [7/12], Loss: 0.8758\n","Epoch [37/100], Step [8/12], Loss: 0.8065\n","Epoch [37/100], Step [9/12], Loss: 0.8285\n","Epoch [37/100], Step [10/12], Loss: 0.8316\n","Epoch [37/100], Step [11/12], Loss: 0.8194\n","Epoch [37/100], Step [12/12], Loss: 0.7107\n","Epoch [38/100], Step [1/12], Loss: 0.8378\n","Epoch [38/100], Step [2/12], Loss: 0.7810\n","Epoch [38/100], Step [3/12], Loss: 0.7442\n","Epoch [38/100], Step [4/12], Loss: 0.8049\n","Epoch [38/100], Step [5/12], Loss: 0.6984\n","Epoch [38/100], Step [6/12], Loss: 0.7404\n","Epoch [38/100], Step [7/12], Loss: 0.8758\n","Epoch [38/100], Step [8/12], Loss: 0.8065\n","Epoch [38/100], Step [9/12], Loss: 0.8285\n","Epoch [38/100], Step [10/12], Loss: 0.8316\n","Epoch [38/100], Step [11/12], Loss: 0.8194\n","Epoch [38/100], Step [12/12], Loss: 0.7107\n","Epoch [39/100], Step [1/12], Loss: 0.8378\n","Epoch [39/100], Step [2/12], Loss: 0.7810\n","Epoch [39/100], Step [3/12], Loss: 0.7442\n","Epoch [39/100], Step [4/12], Loss: 0.8049\n","Epoch [39/100], Step [5/12], Loss: 0.6984\n","Epoch [39/100], Step [6/12], Loss: 0.7404\n","Epoch [39/100], Step [7/12], Loss: 0.8758\n","Epoch [39/100], Step [8/12], Loss: 0.8065\n","Epoch [39/100], Step [9/12], Loss: 0.8285\n","Epoch [39/100], Step [10/12], Loss: 0.8316\n","Epoch [39/100], Step [11/12], Loss: 0.8194\n","Epoch [39/100], Step [12/12], Loss: 0.7107\n","Epoch [40/100], Step [1/12], Loss: 0.8378\n","Epoch [40/100], Step [2/12], Loss: 0.7810\n","Epoch [40/100], Step [3/12], Loss: 0.7442\n","Epoch [40/100], Step [4/12], Loss: 0.8049\n","Epoch [40/100], Step [5/12], Loss: 0.6984\n","Epoch [40/100], Step [6/12], Loss: 0.7404\n","Epoch [40/100], Step [7/12], Loss: 0.8758\n","Epoch [40/100], Step [8/12], Loss: 0.8065\n","Epoch [40/100], Step [9/12], Loss: 0.8285\n","Epoch [40/100], Step [10/12], Loss: 0.8316\n","Epoch [40/100], Step [11/12], Loss: 0.8194\n","Epoch [40/100], Step [12/12], Loss: 0.7107\n","Epoch [41/100], Step [1/12], Loss: 0.8378\n","Epoch [41/100], Step [2/12], Loss: 0.7810\n","Epoch [41/100], Step [3/12], Loss: 0.7442\n","Epoch [41/100], Step [4/12], Loss: 0.8049\n","Epoch [41/100], Step [5/12], Loss: 0.6984\n","Epoch [41/100], Step [6/12], Loss: 0.7404\n","Epoch [41/100], Step [7/12], Loss: 0.8758\n","Epoch [41/100], Step [8/12], Loss: 0.8065\n","Epoch [41/100], Step [9/12], Loss: 0.8285\n","Epoch [41/100], Step [10/12], Loss: 0.8316\n","Epoch [41/100], Step [11/12], Loss: 0.8194\n","Epoch [41/100], Step [12/12], Loss: 0.7107\n","Epoch [42/100], Step [1/12], Loss: 0.8378\n","Epoch [42/100], Step [2/12], Loss: 0.7810\n","Epoch [42/100], Step [3/12], Loss: 0.7442\n","Epoch [42/100], Step [4/12], Loss: 0.8049\n","Epoch [42/100], Step [5/12], Loss: 0.6984\n","Epoch [42/100], Step [6/12], Loss: 0.7404\n","Epoch [42/100], Step [7/12], Loss: 0.8758\n","Epoch [42/100], Step [8/12], Loss: 0.8065\n","Epoch [42/100], Step [9/12], Loss: 0.8285\n","Epoch [42/100], Step [10/12], Loss: 0.8316\n","Epoch [42/100], Step [11/12], Loss: 0.8194\n","Epoch [42/100], Step [12/12], Loss: 0.7107\n","Epoch [43/100], Step [1/12], Loss: 0.8378\n","Epoch [43/100], Step [2/12], Loss: 0.7810\n","Epoch [43/100], Step [3/12], Loss: 0.7442\n","Epoch [43/100], Step [4/12], Loss: 0.8049\n","Epoch [43/100], Step [5/12], Loss: 0.6984\n","Epoch [43/100], Step [6/12], Loss: 0.7404\n","Epoch [43/100], Step [7/12], Loss: 0.8758\n","Epoch [43/100], Step [8/12], Loss: 0.8065\n","Epoch [43/100], Step [9/12], Loss: 0.8285\n","Epoch [43/100], Step [10/12], Loss: 0.8316\n","Epoch [43/100], Step [11/12], Loss: 0.8194\n","Epoch [43/100], Step [12/12], Loss: 0.7107\n","Epoch [44/100], Step [1/12], Loss: 0.8378\n","Epoch [44/100], Step [2/12], Loss: 0.7810\n","Epoch [44/100], Step [3/12], Loss: 0.7442\n","Epoch [44/100], Step [4/12], Loss: 0.8049\n","Epoch [44/100], Step [5/12], Loss: 0.6984\n","Epoch [44/100], Step [6/12], Loss: 0.7404\n","Epoch [44/100], Step [7/12], Loss: 0.8758\n","Epoch [44/100], Step [8/12], Loss: 0.8065\n","Epoch [44/100], Step [9/12], Loss: 0.8285\n","Epoch [44/100], Step [10/12], Loss: 0.8316\n","Epoch [44/100], Step [11/12], Loss: 0.8194\n","Epoch [44/100], Step [12/12], Loss: 0.7107\n","Epoch [45/100], Step [1/12], Loss: 0.8378\n","Epoch [45/100], Step [2/12], Loss: 0.7810\n","Epoch [45/100], Step [3/12], Loss: 0.7442\n","Epoch [45/100], Step [4/12], Loss: 0.8049\n","Epoch [45/100], Step [5/12], Loss: 0.6984\n","Epoch [45/100], Step [6/12], Loss: 0.7404\n","Epoch [45/100], Step [7/12], Loss: 0.8758\n","Epoch [45/100], Step [8/12], Loss: 0.8065\n","Epoch [45/100], Step [9/12], Loss: 0.8285\n","Epoch [45/100], Step [10/12], Loss: 0.8316\n","Epoch [45/100], Step [11/12], Loss: 0.8194\n","Epoch [45/100], Step [12/12], Loss: 0.7107\n","Epoch [46/100], Step [1/12], Loss: 0.8378\n","Epoch [46/100], Step [2/12], Loss: 0.7810\n","Epoch [46/100], Step [3/12], Loss: 0.7442\n","Epoch [46/100], Step [4/12], Loss: 0.8049\n","Epoch [46/100], Step [5/12], Loss: 0.6984\n","Epoch [46/100], Step [6/12], Loss: 0.7404\n","Epoch [46/100], Step [7/12], Loss: 0.8758\n","Epoch [46/100], Step [8/12], Loss: 0.8065\n","Epoch [46/100], Step [9/12], Loss: 0.8285\n","Epoch [46/100], Step [10/12], Loss: 0.8316\n","Epoch [46/100], Step [11/12], Loss: 0.8194\n","Epoch [46/100], Step [12/12], Loss: 0.7107\n","Epoch [47/100], Step [1/12], Loss: 0.8378\n","Epoch [47/100], Step [2/12], Loss: 0.7810\n","Epoch [47/100], Step [3/12], Loss: 0.7442\n","Epoch [47/100], Step [4/12], Loss: 0.8049\n","Epoch [47/100], Step [5/12], Loss: 0.6984\n","Epoch [47/100], Step [6/12], Loss: 0.7404\n","Epoch [47/100], Step [7/12], Loss: 0.8758\n","Epoch [47/100], Step [8/12], Loss: 0.8065\n","Epoch [47/100], Step [9/12], Loss: 0.8285\n","Epoch [47/100], Step [10/12], Loss: 0.8316\n","Epoch [47/100], Step [11/12], Loss: 0.8194\n","Epoch [47/100], Step [12/12], Loss: 0.7107\n","Epoch [48/100], Step [1/12], Loss: 0.8378\n","Epoch [48/100], Step [2/12], Loss: 0.7810\n","Epoch [48/100], Step [3/12], Loss: 0.7442\n","Epoch [48/100], Step [4/12], Loss: 0.8049\n","Epoch [48/100], Step [5/12], Loss: 0.6984\n","Epoch [48/100], Step [6/12], Loss: 0.7404\n","Epoch [48/100], Step [7/12], Loss: 0.8758\n","Epoch [48/100], Step [8/12], Loss: 0.8065\n","Epoch [48/100], Step [9/12], Loss: 0.8285\n","Epoch [48/100], Step [10/12], Loss: 0.8316\n","Epoch [48/100], Step [11/12], Loss: 0.8194\n","Epoch [48/100], Step [12/12], Loss: 0.7107\n","Epoch [49/100], Step [1/12], Loss: 0.8378\n","Epoch [49/100], Step [2/12], Loss: 0.7810\n","Epoch [49/100], Step [3/12], Loss: 0.7442\n","Epoch [49/100], Step [4/12], Loss: 0.8049\n","Epoch [49/100], Step [5/12], Loss: 0.6984\n","Epoch [49/100], Step [6/12], Loss: 0.7404\n","Epoch [49/100], Step [7/12], Loss: 0.8758\n","Epoch [49/100], Step [8/12], Loss: 0.8065\n","Epoch [49/100], Step [9/12], Loss: 0.8285\n","Epoch [49/100], Step [10/12], Loss: 0.8316\n","Epoch [49/100], Step [11/12], Loss: 0.8194\n","Epoch [49/100], Step [12/12], Loss: 0.7107\n","Epoch [50/100], Step [1/12], Loss: 0.8378\n","Epoch [50/100], Step [2/12], Loss: 0.7810\n","Epoch [50/100], Step [3/12], Loss: 0.7442\n","Epoch [50/100], Step [4/12], Loss: 0.8049\n","Epoch [50/100], Step [5/12], Loss: 0.6984\n","Epoch [50/100], Step [6/12], Loss: 0.7404\n","Epoch [50/100], Step [7/12], Loss: 0.8758\n","Epoch [50/100], Step [8/12], Loss: 0.8065\n","Epoch [50/100], Step [9/12], Loss: 0.8285\n","Epoch [50/100], Step [10/12], Loss: 0.8316\n","Epoch [50/100], Step [11/12], Loss: 0.8194\n","Epoch [50/100], Step [12/12], Loss: 0.7107\n","Epoch [51/100], Step [1/12], Loss: 0.8378\n","Epoch [51/100], Step [2/12], Loss: 0.7810\n","Epoch [51/100], Step [3/12], Loss: 0.7442\n","Epoch [51/100], Step [4/12], Loss: 0.8049\n","Epoch [51/100], Step [5/12], Loss: 0.6984\n","Epoch [51/100], Step [6/12], Loss: 0.7404\n","Epoch [51/100], Step [7/12], Loss: 0.8758\n","Epoch [51/100], Step [8/12], Loss: 0.8065\n","Epoch [51/100], Step [9/12], Loss: 0.8285\n","Epoch [51/100], Step [10/12], Loss: 0.8316\n","Epoch [51/100], Step [11/12], Loss: 0.8194\n","Epoch [51/100], Step [12/12], Loss: 0.7107\n","Epoch [52/100], Step [1/12], Loss: 0.8378\n","Epoch [52/100], Step [2/12], Loss: 0.7810\n","Epoch [52/100], Step [3/12], Loss: 0.7442\n","Epoch [52/100], Step [4/12], Loss: 0.8049\n","Epoch [52/100], Step [5/12], Loss: 0.6984\n","Epoch [52/100], Step [6/12], Loss: 0.7404\n","Epoch [52/100], Step [7/12], Loss: 0.8758\n","Epoch [52/100], Step [8/12], Loss: 0.8065\n","Epoch [52/100], Step [9/12], Loss: 0.8285\n","Epoch [52/100], Step [10/12], Loss: 0.8316\n","Epoch [52/100], Step [11/12], Loss: 0.8194\n","Epoch [52/100], Step [12/12], Loss: 0.7107\n","Epoch [53/100], Step [1/12], Loss: 0.8378\n","Epoch [53/100], Step [2/12], Loss: 0.7810\n","Epoch [53/100], Step [3/12], Loss: 0.7442\n","Epoch [53/100], Step [4/12], Loss: 0.8049\n","Epoch [53/100], Step [5/12], Loss: 0.6984\n","Epoch [53/100], Step [6/12], Loss: 0.7404\n","Epoch [53/100], Step [7/12], Loss: 0.8758\n","Epoch [53/100], Step [8/12], Loss: 0.8065\n","Epoch [53/100], Step [9/12], Loss: 0.8285\n","Epoch [53/100], Step [10/12], Loss: 0.8316\n","Epoch [53/100], Step [11/12], Loss: 0.8194\n","Epoch [53/100], Step [12/12], Loss: 0.7107\n","Epoch [54/100], Step [1/12], Loss: 0.8378\n","Epoch [54/100], Step [2/12], Loss: 0.7810\n","Epoch [54/100], Step [3/12], Loss: 0.7442\n","Epoch [54/100], Step [4/12], Loss: 0.8049\n","Epoch [54/100], Step [5/12], Loss: 0.6984\n","Epoch [54/100], Step [6/12], Loss: 0.7404\n","Epoch [54/100], Step [7/12], Loss: 0.8758\n","Epoch [54/100], Step [8/12], Loss: 0.8065\n","Epoch [54/100], Step [9/12], Loss: 0.8285\n","Epoch [54/100], Step [10/12], Loss: 0.8316\n","Epoch [54/100], Step [11/12], Loss: 0.8194\n","Epoch [54/100], Step [12/12], Loss: 0.7107\n","Epoch [55/100], Step [1/12], Loss: 0.8378\n","Epoch [55/100], Step [2/12], Loss: 0.7810\n","Epoch [55/100], Step [3/12], Loss: 0.7442\n","Epoch [55/100], Step [4/12], Loss: 0.8049\n","Epoch [55/100], Step [5/12], Loss: 0.6984\n","Epoch [55/100], Step [6/12], Loss: 0.7404\n","Epoch [55/100], Step [7/12], Loss: 0.8758\n","Epoch [55/100], Step [8/12], Loss: 0.8065\n","Epoch [55/100], Step [9/12], Loss: 0.8285\n","Epoch [55/100], Step [10/12], Loss: 0.8316\n","Epoch [55/100], Step [11/12], Loss: 0.8194\n","Epoch [55/100], Step [12/12], Loss: 0.7107\n","Epoch [56/100], Step [1/12], Loss: 0.8378\n","Epoch [56/100], Step [2/12], Loss: 0.7810\n","Epoch [56/100], Step [3/12], Loss: 0.7442\n","Epoch [56/100], Step [4/12], Loss: 0.8049\n","Epoch [56/100], Step [5/12], Loss: 0.6984\n","Epoch [56/100], Step [6/12], Loss: 0.7404\n","Epoch [56/100], Step [7/12], Loss: 0.8758\n","Epoch [56/100], Step [8/12], Loss: 0.8065\n","Epoch [56/100], Step [9/12], Loss: 0.8285\n","Epoch [56/100], Step [10/12], Loss: 0.8316\n","Epoch [56/100], Step [11/12], Loss: 0.8194\n","Epoch [56/100], Step [12/12], Loss: 0.7107\n","Epoch [57/100], Step [1/12], Loss: 0.8378\n","Epoch [57/100], Step [2/12], Loss: 0.7810\n","Epoch [57/100], Step [3/12], Loss: 0.7442\n","Epoch [57/100], Step [4/12], Loss: 0.8049\n","Epoch [57/100], Step [5/12], Loss: 0.6984\n","Epoch [57/100], Step [6/12], Loss: 0.7404\n","Epoch [57/100], Step [7/12], Loss: 0.8758\n","Epoch [57/100], Step [8/12], Loss: 0.8065\n","Epoch [57/100], Step [9/12], Loss: 0.8285\n","Epoch [57/100], Step [10/12], Loss: 0.8316\n","Epoch [57/100], Step [11/12], Loss: 0.8194\n","Epoch [57/100], Step [12/12], Loss: 0.7107\n","Epoch [58/100], Step [1/12], Loss: 0.8378\n","Epoch [58/100], Step [2/12], Loss: 0.7810\n","Epoch [58/100], Step [3/12], Loss: 0.7442\n","Epoch [58/100], Step [4/12], Loss: 0.8049\n","Epoch [58/100], Step [5/12], Loss: 0.6984\n","Epoch [58/100], Step [6/12], Loss: 0.7404\n","Epoch [58/100], Step [7/12], Loss: 0.8758\n","Epoch [58/100], Step [8/12], Loss: 0.8065\n","Epoch [58/100], Step [9/12], Loss: 0.8285\n","Epoch [58/100], Step [10/12], Loss: 0.8316\n","Epoch [58/100], Step [11/12], Loss: 0.8194\n","Epoch [58/100], Step [12/12], Loss: 0.7107\n","Epoch [59/100], Step [1/12], Loss: 0.8378\n","Epoch [59/100], Step [2/12], Loss: 0.7810\n","Epoch [59/100], Step [3/12], Loss: 0.7442\n","Epoch [59/100], Step [4/12], Loss: 0.8049\n","Epoch [59/100], Step [5/12], Loss: 0.6984\n","Epoch [59/100], Step [6/12], Loss: 0.7404\n","Epoch [59/100], Step [7/12], Loss: 0.8758\n","Epoch [59/100], Step [8/12], Loss: 0.8065\n","Epoch [59/100], Step [9/12], Loss: 0.8285\n","Epoch [59/100], Step [10/12], Loss: 0.8316\n","Epoch [59/100], Step [11/12], Loss: 0.8194\n","Epoch [59/100], Step [12/12], Loss: 0.7107\n","Epoch [60/100], Step [1/12], Loss: 0.8378\n","Epoch [60/100], Step [2/12], Loss: 0.7810\n","Epoch [60/100], Step [3/12], Loss: 0.7442\n","Epoch [60/100], Step [4/12], Loss: 0.8049\n","Epoch [60/100], Step [5/12], Loss: 0.6984\n","Epoch [60/100], Step [6/12], Loss: 0.7404\n","Epoch [60/100], Step [7/12], Loss: 0.8758\n","Epoch [60/100], Step [8/12], Loss: 0.8065\n","Epoch [60/100], Step [9/12], Loss: 0.8285\n","Epoch [60/100], Step [10/12], Loss: 0.8316\n","Epoch [60/100], Step [11/12], Loss: 0.8194\n","Epoch [60/100], Step [12/12], Loss: 0.7107\n","Epoch [61/100], Step [1/12], Loss: 0.8378\n","Epoch [61/100], Step [2/12], Loss: 0.7810\n","Epoch [61/100], Step [3/12], Loss: 0.7442\n","Epoch [61/100], Step [4/12], Loss: 0.8049\n","Epoch [61/100], Step [5/12], Loss: 0.6984\n","Epoch [61/100], Step [6/12], Loss: 0.7404\n","Epoch [61/100], Step [7/12], Loss: 0.8758\n","Epoch [61/100], Step [8/12], Loss: 0.8065\n","Epoch [61/100], Step [9/12], Loss: 0.8285\n","Epoch [61/100], Step [10/12], Loss: 0.8316\n","Epoch [61/100], Step [11/12], Loss: 0.8194\n","Epoch [61/100], Step [12/12], Loss: 0.7107\n","Epoch [62/100], Step [1/12], Loss: 0.8378\n","Epoch [62/100], Step [2/12], Loss: 0.7810\n","Epoch [62/100], Step [3/12], Loss: 0.7442\n","Epoch [62/100], Step [4/12], Loss: 0.8049\n","Epoch [62/100], Step [5/12], Loss: 0.6984\n","Epoch [62/100], Step [6/12], Loss: 0.7404\n","Epoch [62/100], Step [7/12], Loss: 0.8758\n","Epoch [62/100], Step [8/12], Loss: 0.8065\n","Epoch [62/100], Step [9/12], Loss: 0.8285\n","Epoch [62/100], Step [10/12], Loss: 0.8316\n","Epoch [62/100], Step [11/12], Loss: 0.8194\n","Epoch [62/100], Step [12/12], Loss: 0.7107\n","Epoch [63/100], Step [1/12], Loss: 0.8378\n","Epoch [63/100], Step [2/12], Loss: 0.7810\n","Epoch [63/100], Step [3/12], Loss: 0.7442\n","Epoch [63/100], Step [4/12], Loss: 0.8049\n","Epoch [63/100], Step [5/12], Loss: 0.6984\n","Epoch [63/100], Step [6/12], Loss: 0.7404\n","Epoch [63/100], Step [7/12], Loss: 0.8758\n","Epoch [63/100], Step [8/12], Loss: 0.8065\n","Epoch [63/100], Step [9/12], Loss: 0.8285\n","Epoch [63/100], Step [10/12], Loss: 0.8316\n","Epoch [63/100], Step [11/12], Loss: 0.8194\n","Epoch [63/100], Step [12/12], Loss: 0.7107\n","Epoch [64/100], Step [1/12], Loss: 0.8378\n","Epoch [64/100], Step [2/12], Loss: 0.7810\n","Epoch [64/100], Step [3/12], Loss: 0.7442\n","Epoch [64/100], Step [4/12], Loss: 0.8049\n","Epoch [64/100], Step [5/12], Loss: 0.6984\n","Epoch [64/100], Step [6/12], Loss: 0.7404\n","Epoch [64/100], Step [7/12], Loss: 0.8758\n","Epoch [64/100], Step [8/12], Loss: 0.8065\n","Epoch [64/100], Step [9/12], Loss: 0.8285\n","Epoch [64/100], Step [10/12], Loss: 0.8316\n","Epoch [64/100], Step [11/12], Loss: 0.8194\n","Epoch [64/100], Step [12/12], Loss: 0.7107\n","Epoch [65/100], Step [1/12], Loss: 0.8378\n","Epoch [65/100], Step [2/12], Loss: 0.7810\n","Epoch [65/100], Step [3/12], Loss: 0.7442\n","Epoch [65/100], Step [4/12], Loss: 0.8049\n","Epoch [65/100], Step [5/12], Loss: 0.6984\n","Epoch [65/100], Step [6/12], Loss: 0.7404\n","Epoch [65/100], Step [7/12], Loss: 0.8758\n","Epoch [65/100], Step [8/12], Loss: 0.8065\n","Epoch [65/100], Step [9/12], Loss: 0.8285\n","Epoch [65/100], Step [10/12], Loss: 0.8316\n","Epoch [65/100], Step [11/12], Loss: 0.8194\n","Epoch [65/100], Step [12/12], Loss: 0.7107\n","Epoch [66/100], Step [1/12], Loss: 0.8378\n","Epoch [66/100], Step [2/12], Loss: 0.7810\n","Epoch [66/100], Step [3/12], Loss: 0.7442\n","Epoch [66/100], Step [4/12], Loss: 0.8049\n","Epoch [66/100], Step [5/12], Loss: 0.6984\n","Epoch [66/100], Step [6/12], Loss: 0.7404\n","Epoch [66/100], Step [7/12], Loss: 0.8758\n","Epoch [66/100], Step [8/12], Loss: 0.8065\n","Epoch [66/100], Step [9/12], Loss: 0.8285\n","Epoch [66/100], Step [10/12], Loss: 0.8316\n","Epoch [66/100], Step [11/12], Loss: 0.8194\n","Epoch [66/100], Step [12/12], Loss: 0.7107\n","Epoch [67/100], Step [1/12], Loss: 0.8378\n","Epoch [67/100], Step [2/12], Loss: 0.7810\n","Epoch [67/100], Step [3/12], Loss: 0.7442\n","Epoch [67/100], Step [4/12], Loss: 0.8049\n","Epoch [67/100], Step [5/12], Loss: 0.6984\n","Epoch [67/100], Step [6/12], Loss: 0.7404\n","Epoch [67/100], Step [7/12], Loss: 0.8758\n","Epoch [67/100], Step [8/12], Loss: 0.8065\n","Epoch [67/100], Step [9/12], Loss: 0.8285\n","Epoch [67/100], Step [10/12], Loss: 0.8316\n","Epoch [67/100], Step [11/12], Loss: 0.8194\n","Epoch [67/100], Step [12/12], Loss: 0.7107\n","Epoch [68/100], Step [1/12], Loss: 0.8378\n","Epoch [68/100], Step [2/12], Loss: 0.7810\n","Epoch [68/100], Step [3/12], Loss: 0.7442\n","Epoch [68/100], Step [4/12], Loss: 0.8049\n","Epoch [68/100], Step [5/12], Loss: 0.6984\n","Epoch [68/100], Step [6/12], Loss: 0.7404\n","Epoch [68/100], Step [7/12], Loss: 0.8758\n","Epoch [68/100], Step [8/12], Loss: 0.8065\n","Epoch [68/100], Step [9/12], Loss: 0.8285\n","Epoch [68/100], Step [10/12], Loss: 0.8316\n","Epoch [68/100], Step [11/12], Loss: 0.8194\n","Epoch [68/100], Step [12/12], Loss: 0.7107\n","Epoch [69/100], Step [1/12], Loss: 0.8378\n","Epoch [69/100], Step [2/12], Loss: 0.7810\n","Epoch [69/100], Step [3/12], Loss: 0.7442\n","Epoch [69/100], Step [4/12], Loss: 0.8049\n","Epoch [69/100], Step [5/12], Loss: 0.6984\n","Epoch [69/100], Step [6/12], Loss: 0.7404\n","Epoch [69/100], Step [7/12], Loss: 0.8758\n","Epoch [69/100], Step [8/12], Loss: 0.8065\n","Epoch [69/100], Step [9/12], Loss: 0.8285\n","Epoch [69/100], Step [10/12], Loss: 0.8316\n","Epoch [69/100], Step [11/12], Loss: 0.8194\n","Epoch [69/100], Step [12/12], Loss: 0.7107\n","Epoch [70/100], Step [1/12], Loss: 0.8378\n","Epoch [70/100], Step [2/12], Loss: 0.7810\n","Epoch [70/100], Step [3/12], Loss: 0.7442\n","Epoch [70/100], Step [4/12], Loss: 0.8049\n","Epoch [70/100], Step [5/12], Loss: 0.6984\n","Epoch [70/100], Step [6/12], Loss: 0.7404\n","Epoch [70/100], Step [7/12], Loss: 0.8758\n","Epoch [70/100], Step [8/12], Loss: 0.8065\n","Epoch [70/100], Step [9/12], Loss: 0.8285\n","Epoch [70/100], Step [10/12], Loss: 0.8316\n","Epoch [70/100], Step [11/12], Loss: 0.8194\n","Epoch [70/100], Step [12/12], Loss: 0.7107\n","Epoch [71/100], Step [1/12], Loss: 0.8378\n","Epoch [71/100], Step [2/12], Loss: 0.7810\n","Epoch [71/100], Step [3/12], Loss: 0.7442\n","Epoch [71/100], Step [4/12], Loss: 0.8049\n","Epoch [71/100], Step [5/12], Loss: 0.6984\n","Epoch [71/100], Step [6/12], Loss: 0.7404\n","Epoch [71/100], Step [7/12], Loss: 0.8758\n","Epoch [71/100], Step [8/12], Loss: 0.8065\n","Epoch [71/100], Step [9/12], Loss: 0.8285\n","Epoch [71/100], Step [10/12], Loss: 0.8316\n","Epoch [71/100], Step [11/12], Loss: 0.8194\n","Epoch [71/100], Step [12/12], Loss: 0.7107\n","Epoch [72/100], Step [1/12], Loss: 0.8378\n","Epoch [72/100], Step [2/12], Loss: 0.7810\n","Epoch [72/100], Step [3/12], Loss: 0.7442\n","Epoch [72/100], Step [4/12], Loss: 0.8049\n","Epoch [72/100], Step [5/12], Loss: 0.6984\n","Epoch [72/100], Step [6/12], Loss: 0.7404\n","Epoch [72/100], Step [7/12], Loss: 0.8758\n","Epoch [72/100], Step [8/12], Loss: 0.8065\n","Epoch [72/100], Step [9/12], Loss: 0.8285\n","Epoch [72/100], Step [10/12], Loss: 0.8316\n","Epoch [72/100], Step [11/12], Loss: 0.8194\n","Epoch [72/100], Step [12/12], Loss: 0.7107\n","Epoch [73/100], Step [1/12], Loss: 0.8378\n","Epoch [73/100], Step [2/12], Loss: 0.7810\n","Epoch [73/100], Step [3/12], Loss: 0.7442\n","Epoch [73/100], Step [4/12], Loss: 0.8049\n","Epoch [73/100], Step [5/12], Loss: 0.6984\n","Epoch [73/100], Step [6/12], Loss: 0.7404\n","Epoch [73/100], Step [7/12], Loss: 0.8758\n","Epoch [73/100], Step [8/12], Loss: 0.8065\n","Epoch [73/100], Step [9/12], Loss: 0.8285\n","Epoch [73/100], Step [10/12], Loss: 0.8316\n","Epoch [73/100], Step [11/12], Loss: 0.8194\n","Epoch [73/100], Step [12/12], Loss: 0.7107\n","Epoch [74/100], Step [1/12], Loss: 0.8378\n","Epoch [74/100], Step [2/12], Loss: 0.7810\n","Epoch [74/100], Step [3/12], Loss: 0.7442\n","Epoch [74/100], Step [4/12], Loss: 0.8049\n","Epoch [74/100], Step [5/12], Loss: 0.6984\n","Epoch [74/100], Step [6/12], Loss: 0.7404\n","Epoch [74/100], Step [7/12], Loss: 0.8758\n","Epoch [74/100], Step [8/12], Loss: 0.8065\n","Epoch [74/100], Step [9/12], Loss: 0.8285\n","Epoch [74/100], Step [10/12], Loss: 0.8316\n","Epoch [74/100], Step [11/12], Loss: 0.8194\n","Epoch [74/100], Step [12/12], Loss: 0.7107\n","Epoch [75/100], Step [1/12], Loss: 0.8378\n","Epoch [75/100], Step [2/12], Loss: 0.7810\n","Epoch [75/100], Step [3/12], Loss: 0.7442\n","Epoch [75/100], Step [4/12], Loss: 0.8049\n","Epoch [75/100], Step [5/12], Loss: 0.6984\n","Epoch [75/100], Step [6/12], Loss: 0.7404\n","Epoch [75/100], Step [7/12], Loss: 0.8758\n","Epoch [75/100], Step [8/12], Loss: 0.8065\n","Epoch [75/100], Step [9/12], Loss: 0.8285\n","Epoch [75/100], Step [10/12], Loss: 0.8316\n","Epoch [75/100], Step [11/12], Loss: 0.8194\n","Epoch [75/100], Step [12/12], Loss: 0.7107\n","Epoch [76/100], Step [1/12], Loss: 0.8378\n","Epoch [76/100], Step [2/12], Loss: 0.7810\n","Epoch [76/100], Step [3/12], Loss: 0.7442\n","Epoch [76/100], Step [4/12], Loss: 0.8049\n","Epoch [76/100], Step [5/12], Loss: 0.6984\n","Epoch [76/100], Step [6/12], Loss: 0.7404\n","Epoch [76/100], Step [7/12], Loss: 0.8758\n","Epoch [76/100], Step [8/12], Loss: 0.8065\n","Epoch [76/100], Step [9/12], Loss: 0.8285\n","Epoch [76/100], Step [10/12], Loss: 0.8316\n","Epoch [76/100], Step [11/12], Loss: 0.8194\n","Epoch [76/100], Step [12/12], Loss: 0.7107\n","Epoch [77/100], Step [1/12], Loss: 0.8378\n","Epoch [77/100], Step [2/12], Loss: 0.7810\n","Epoch [77/100], Step [3/12], Loss: 0.7442\n","Epoch [77/100], Step [4/12], Loss: 0.8049\n","Epoch [77/100], Step [5/12], Loss: 0.6984\n","Epoch [77/100], Step [6/12], Loss: 0.7404\n","Epoch [77/100], Step [7/12], Loss: 0.8758\n","Epoch [77/100], Step [8/12], Loss: 0.8065\n","Epoch [77/100], Step [9/12], Loss: 0.8285\n","Epoch [77/100], Step [10/12], Loss: 0.8316\n","Epoch [77/100], Step [11/12], Loss: 0.8194\n","Epoch [77/100], Step [12/12], Loss: 0.7107\n","Epoch [78/100], Step [1/12], Loss: 0.8378\n","Epoch [78/100], Step [2/12], Loss: 0.7810\n","Epoch [78/100], Step [3/12], Loss: 0.7442\n","Epoch [78/100], Step [4/12], Loss: 0.8049\n","Epoch [78/100], Step [5/12], Loss: 0.6984\n","Epoch [78/100], Step [6/12], Loss: 0.7404\n","Epoch [78/100], Step [7/12], Loss: 0.8758\n","Epoch [78/100], Step [8/12], Loss: 0.8065\n","Epoch [78/100], Step [9/12], Loss: 0.8285\n","Epoch [78/100], Step [10/12], Loss: 0.8316\n","Epoch [78/100], Step [11/12], Loss: 0.8194\n","Epoch [78/100], Step [12/12], Loss: 0.7107\n","Epoch [79/100], Step [1/12], Loss: 0.8378\n","Epoch [79/100], Step [2/12], Loss: 0.7810\n","Epoch [79/100], Step [3/12], Loss: 0.7442\n","Epoch [79/100], Step [4/12], Loss: 0.8049\n","Epoch [79/100], Step [5/12], Loss: 0.6984\n","Epoch [79/100], Step [6/12], Loss: 0.7404\n","Epoch [79/100], Step [7/12], Loss: 0.8758\n","Epoch [79/100], Step [8/12], Loss: 0.8065\n","Epoch [79/100], Step [9/12], Loss: 0.8285\n","Epoch [79/100], Step [10/12], Loss: 0.8316\n","Epoch [79/100], Step [11/12], Loss: 0.8194\n","Epoch [79/100], Step [12/12], Loss: 0.7107\n","Epoch [80/100], Step [1/12], Loss: 0.8378\n","Epoch [80/100], Step [2/12], Loss: 0.7810\n","Epoch [80/100], Step [3/12], Loss: 0.7442\n","Epoch [80/100], Step [4/12], Loss: 0.8049\n","Epoch [80/100], Step [5/12], Loss: 0.6984\n","Epoch [80/100], Step [6/12], Loss: 0.7404\n","Epoch [80/100], Step [7/12], Loss: 0.8758\n","Epoch [80/100], Step [8/12], Loss: 0.8065\n","Epoch [80/100], Step [9/12], Loss: 0.8285\n","Epoch [80/100], Step [10/12], Loss: 0.8316\n","Epoch [80/100], Step [11/12], Loss: 0.8194\n","Epoch [80/100], Step [12/12], Loss: 0.7107\n","Epoch [81/100], Step [1/12], Loss: 0.8378\n","Epoch [81/100], Step [2/12], Loss: 0.7810\n","Epoch [81/100], Step [3/12], Loss: 0.7442\n","Epoch [81/100], Step [4/12], Loss: 0.8049\n","Epoch [81/100], Step [5/12], Loss: 0.6984\n","Epoch [81/100], Step [6/12], Loss: 0.7404\n","Epoch [81/100], Step [7/12], Loss: 0.8758\n","Epoch [81/100], Step [8/12], Loss: 0.8065\n","Epoch [81/100], Step [9/12], Loss: 0.8285\n","Epoch [81/100], Step [10/12], Loss: 0.8316\n","Epoch [81/100], Step [11/12], Loss: 0.8194\n","Epoch [81/100], Step [12/12], Loss: 0.7107\n","Epoch [82/100], Step [1/12], Loss: 0.8378\n","Epoch [82/100], Step [2/12], Loss: 0.7810\n","Epoch [82/100], Step [3/12], Loss: 0.7442\n","Epoch [82/100], Step [4/12], Loss: 0.8049\n","Epoch [82/100], Step [5/12], Loss: 0.6984\n","Epoch [82/100], Step [6/12], Loss: 0.7404\n","Epoch [82/100], Step [7/12], Loss: 0.8758\n","Epoch [82/100], Step [8/12], Loss: 0.8065\n","Epoch [82/100], Step [9/12], Loss: 0.8285\n","Epoch [82/100], Step [10/12], Loss: 0.8316\n","Epoch [82/100], Step [11/12], Loss: 0.8194\n","Epoch [82/100], Step [12/12], Loss: 0.7107\n","Epoch [83/100], Step [1/12], Loss: 0.8378\n","Epoch [83/100], Step [2/12], Loss: 0.7810\n","Epoch [83/100], Step [3/12], Loss: 0.7442\n","Epoch [83/100], Step [4/12], Loss: 0.8049\n","Epoch [83/100], Step [5/12], Loss: 0.6984\n","Epoch [83/100], Step [6/12], Loss: 0.7404\n","Epoch [83/100], Step [7/12], Loss: 0.8758\n","Epoch [83/100], Step [8/12], Loss: 0.8065\n","Epoch [83/100], Step [9/12], Loss: 0.8285\n","Epoch [83/100], Step [10/12], Loss: 0.8316\n","Epoch [83/100], Step [11/12], Loss: 0.8194\n","Epoch [83/100], Step [12/12], Loss: 0.7107\n","Epoch [84/100], Step [1/12], Loss: 0.8378\n","Epoch [84/100], Step [2/12], Loss: 0.7810\n","Epoch [84/100], Step [3/12], Loss: 0.7442\n","Epoch [84/100], Step [4/12], Loss: 0.8049\n","Epoch [84/100], Step [5/12], Loss: 0.6984\n","Epoch [84/100], Step [6/12], Loss: 0.7404\n","Epoch [84/100], Step [7/12], Loss: 0.8758\n","Epoch [84/100], Step [8/12], Loss: 0.8065\n","Epoch [84/100], Step [9/12], Loss: 0.8285\n","Epoch [84/100], Step [10/12], Loss: 0.8316\n","Epoch [84/100], Step [11/12], Loss: 0.8194\n","Epoch [84/100], Step [12/12], Loss: 0.7107\n","Epoch [85/100], Step [1/12], Loss: 0.8378\n","Epoch [85/100], Step [2/12], Loss: 0.7810\n","Epoch [85/100], Step [3/12], Loss: 0.7442\n","Epoch [85/100], Step [4/12], Loss: 0.8049\n","Epoch [85/100], Step [5/12], Loss: 0.6984\n","Epoch [85/100], Step [6/12], Loss: 0.7404\n","Epoch [85/100], Step [7/12], Loss: 0.8758\n","Epoch [85/100], Step [8/12], Loss: 0.8065\n","Epoch [85/100], Step [9/12], Loss: 0.8285\n","Epoch [85/100], Step [10/12], Loss: 0.8316\n","Epoch [85/100], Step [11/12], Loss: 0.8194\n","Epoch [85/100], Step [12/12], Loss: 0.7107\n","Epoch [86/100], Step [1/12], Loss: 0.8378\n","Epoch [86/100], Step [2/12], Loss: 0.7810\n","Epoch [86/100], Step [3/12], Loss: 0.7442\n","Epoch [86/100], Step [4/12], Loss: 0.8049\n","Epoch [86/100], Step [5/12], Loss: 0.6984\n","Epoch [86/100], Step [6/12], Loss: 0.7404\n","Epoch [86/100], Step [7/12], Loss: 0.8758\n","Epoch [86/100], Step [8/12], Loss: 0.8065\n","Epoch [86/100], Step [9/12], Loss: 0.8285\n","Epoch [86/100], Step [10/12], Loss: 0.8316\n","Epoch [86/100], Step [11/12], Loss: 0.8194\n","Epoch [86/100], Step [12/12], Loss: 0.7107\n","Epoch [87/100], Step [1/12], Loss: 0.8378\n","Epoch [87/100], Step [2/12], Loss: 0.7810\n","Epoch [87/100], Step [3/12], Loss: 0.7442\n","Epoch [87/100], Step [4/12], Loss: 0.8049\n","Epoch [87/100], Step [5/12], Loss: 0.6984\n","Epoch [87/100], Step [6/12], Loss: 0.7404\n","Epoch [87/100], Step [7/12], Loss: 0.8758\n","Epoch [87/100], Step [8/12], Loss: 0.8065\n","Epoch [87/100], Step [9/12], Loss: 0.8285\n","Epoch [87/100], Step [10/12], Loss: 0.8316\n","Epoch [87/100], Step [11/12], Loss: 0.8194\n","Epoch [87/100], Step [12/12], Loss: 0.7107\n","Epoch [88/100], Step [1/12], Loss: 0.8378\n","Epoch [88/100], Step [2/12], Loss: 0.7810\n","Epoch [88/100], Step [3/12], Loss: 0.7442\n","Epoch [88/100], Step [4/12], Loss: 0.8049\n","Epoch [88/100], Step [5/12], Loss: 0.6984\n","Epoch [88/100], Step [6/12], Loss: 0.7404\n","Epoch [88/100], Step [7/12], Loss: 0.8758\n","Epoch [88/100], Step [8/12], Loss: 0.8065\n","Epoch [88/100], Step [9/12], Loss: 0.8285\n","Epoch [88/100], Step [10/12], Loss: 0.8316\n","Epoch [88/100], Step [11/12], Loss: 0.8194\n","Epoch [88/100], Step [12/12], Loss: 0.7107\n","Epoch [89/100], Step [1/12], Loss: 0.8378\n","Epoch [89/100], Step [2/12], Loss: 0.7810\n","Epoch [89/100], Step [3/12], Loss: 0.7442\n","Epoch [89/100], Step [4/12], Loss: 0.8049\n","Epoch [89/100], Step [5/12], Loss: 0.6984\n","Epoch [89/100], Step [6/12], Loss: 0.7404\n","Epoch [89/100], Step [7/12], Loss: 0.8758\n","Epoch [89/100], Step [8/12], Loss: 0.8065\n","Epoch [89/100], Step [9/12], Loss: 0.8285\n","Epoch [89/100], Step [10/12], Loss: 0.8316\n","Epoch [89/100], Step [11/12], Loss: 0.8194\n","Epoch [89/100], Step [12/12], Loss: 0.7107\n","Epoch [90/100], Step [1/12], Loss: 0.8378\n","Epoch [90/100], Step [2/12], Loss: 0.7810\n","Epoch [90/100], Step [3/12], Loss: 0.7442\n","Epoch [90/100], Step [4/12], Loss: 0.8049\n","Epoch [90/100], Step [5/12], Loss: 0.6984\n","Epoch [90/100], Step [6/12], Loss: 0.7404\n","Epoch [90/100], Step [7/12], Loss: 0.8758\n","Epoch [90/100], Step [8/12], Loss: 0.8065\n","Epoch [90/100], Step [9/12], Loss: 0.8285\n","Epoch [90/100], Step [10/12], Loss: 0.8316\n","Epoch [90/100], Step [11/12], Loss: 0.8194\n","Epoch [90/100], Step [12/12], Loss: 0.7107\n","Epoch [91/100], Step [1/12], Loss: 0.8378\n","Epoch [91/100], Step [2/12], Loss: 0.7810\n","Epoch [91/100], Step [3/12], Loss: 0.7442\n","Epoch [91/100], Step [4/12], Loss: 0.8049\n","Epoch [91/100], Step [5/12], Loss: 0.6984\n","Epoch [91/100], Step [6/12], Loss: 0.7404\n","Epoch [91/100], Step [7/12], Loss: 0.8758\n","Epoch [91/100], Step [8/12], Loss: 0.8065\n","Epoch [91/100], Step [9/12], Loss: 0.8285\n","Epoch [91/100], Step [10/12], Loss: 0.8316\n","Epoch [91/100], Step [11/12], Loss: 0.8194\n","Epoch [91/100], Step [12/12], Loss: 0.7107\n","Epoch [92/100], Step [1/12], Loss: 0.8378\n","Epoch [92/100], Step [2/12], Loss: 0.7810\n","Epoch [92/100], Step [3/12], Loss: 0.7442\n","Epoch [92/100], Step [4/12], Loss: 0.8049\n","Epoch [92/100], Step [5/12], Loss: 0.6984\n","Epoch [92/100], Step [6/12], Loss: 0.7404\n","Epoch [92/100], Step [7/12], Loss: 0.8758\n","Epoch [92/100], Step [8/12], Loss: 0.8065\n","Epoch [92/100], Step [9/12], Loss: 0.8285\n","Epoch [92/100], Step [10/12], Loss: 0.8316\n","Epoch [92/100], Step [11/12], Loss: 0.8194\n","Epoch [92/100], Step [12/12], Loss: 0.7107\n","Epoch [93/100], Step [1/12], Loss: 0.8378\n","Epoch [93/100], Step [2/12], Loss: 0.7810\n","Epoch [93/100], Step [3/12], Loss: 0.7442\n","Epoch [93/100], Step [4/12], Loss: 0.8049\n","Epoch [93/100], Step [5/12], Loss: 0.6984\n","Epoch [93/100], Step [6/12], Loss: 0.7404\n","Epoch [93/100], Step [7/12], Loss: 0.8758\n","Epoch [93/100], Step [8/12], Loss: 0.8065\n","Epoch [93/100], Step [9/12], Loss: 0.8285\n","Epoch [93/100], Step [10/12], Loss: 0.8316\n","Epoch [93/100], Step [11/12], Loss: 0.8194\n","Epoch [93/100], Step [12/12], Loss: 0.7107\n","Epoch [94/100], Step [1/12], Loss: 0.8378\n","Epoch [94/100], Step [2/12], Loss: 0.7810\n","Epoch [94/100], Step [3/12], Loss: 0.7442\n","Epoch [94/100], Step [4/12], Loss: 0.8049\n","Epoch [94/100], Step [5/12], Loss: 0.6984\n","Epoch [94/100], Step [6/12], Loss: 0.7404\n","Epoch [94/100], Step [7/12], Loss: 0.8758\n","Epoch [94/100], Step [8/12], Loss: 0.8065\n","Epoch [94/100], Step [9/12], Loss: 0.8285\n","Epoch [94/100], Step [10/12], Loss: 0.8316\n","Epoch [94/100], Step [11/12], Loss: 0.8194\n","Epoch [94/100], Step [12/12], Loss: 0.7107\n","Epoch [95/100], Step [1/12], Loss: 0.8378\n","Epoch [95/100], Step [2/12], Loss: 0.7810\n","Epoch [95/100], Step [3/12], Loss: 0.7442\n","Epoch [95/100], Step [4/12], Loss: 0.8049\n","Epoch [95/100], Step [5/12], Loss: 0.6984\n","Epoch [95/100], Step [6/12], Loss: 0.7404\n","Epoch [95/100], Step [7/12], Loss: 0.8758\n","Epoch [95/100], Step [8/12], Loss: 0.8065\n","Epoch [95/100], Step [9/12], Loss: 0.8285\n","Epoch [95/100], Step [10/12], Loss: 0.8316\n","Epoch [95/100], Step [11/12], Loss: 0.8194\n","Epoch [95/100], Step [12/12], Loss: 0.7107\n","Epoch [96/100], Step [1/12], Loss: 0.8378\n","Epoch [96/100], Step [2/12], Loss: 0.7810\n","Epoch [96/100], Step [3/12], Loss: 0.7442\n","Epoch [96/100], Step [4/12], Loss: 0.8049\n","Epoch [96/100], Step [5/12], Loss: 0.6984\n","Epoch [96/100], Step [6/12], Loss: 0.7404\n","Epoch [96/100], Step [7/12], Loss: 0.8758\n","Epoch [96/100], Step [8/12], Loss: 0.8065\n","Epoch [96/100], Step [9/12], Loss: 0.8285\n","Epoch [96/100], Step [10/12], Loss: 0.8316\n","Epoch [96/100], Step [11/12], Loss: 0.8194\n","Epoch [96/100], Step [12/12], Loss: 0.7107\n","Epoch [97/100], Step [1/12], Loss: 0.8378\n","Epoch [97/100], Step [2/12], Loss: 0.7810\n","Epoch [97/100], Step [3/12], Loss: 0.7442\n","Epoch [97/100], Step [4/12], Loss: 0.8049\n","Epoch [97/100], Step [5/12], Loss: 0.6984\n","Epoch [97/100], Step [6/12], Loss: 0.7404\n","Epoch [97/100], Step [7/12], Loss: 0.8758\n","Epoch [97/100], Step [8/12], Loss: 0.8065\n","Epoch [97/100], Step [9/12], Loss: 0.8285\n","Epoch [97/100], Step [10/12], Loss: 0.8316\n","Epoch [97/100], Step [11/12], Loss: 0.8194\n","Epoch [97/100], Step [12/12], Loss: 0.7107\n","Epoch [98/100], Step [1/12], Loss: 0.8378\n","Epoch [98/100], Step [2/12], Loss: 0.7810\n","Epoch [98/100], Step [3/12], Loss: 0.7442\n","Epoch [98/100], Step [4/12], Loss: 0.8049\n","Epoch [98/100], Step [5/12], Loss: 0.6984\n","Epoch [98/100], Step [6/12], Loss: 0.7404\n","Epoch [98/100], Step [7/12], Loss: 0.8758\n","Epoch [98/100], Step [8/12], Loss: 0.8065\n","Epoch [98/100], Step [9/12], Loss: 0.8285\n","Epoch [98/100], Step [10/12], Loss: 0.8316\n","Epoch [98/100], Step [11/12], Loss: 0.8194\n","Epoch [98/100], Step [12/12], Loss: 0.7107\n","Epoch [99/100], Step [1/12], Loss: 0.8378\n","Epoch [99/100], Step [2/12], Loss: 0.7810\n","Epoch [99/100], Step [3/12], Loss: 0.7442\n","Epoch [99/100], Step [4/12], Loss: 0.8049\n","Epoch [99/100], Step [5/12], Loss: 0.6984\n","Epoch [99/100], Step [6/12], Loss: 0.7404\n","Epoch [99/100], Step [7/12], Loss: 0.8758\n","Epoch [99/100], Step [8/12], Loss: 0.8065\n","Epoch [99/100], Step [9/12], Loss: 0.8285\n","Epoch [99/100], Step [10/12], Loss: 0.8316\n","Epoch [99/100], Step [11/12], Loss: 0.8194\n","Epoch [99/100], Step [12/12], Loss: 0.7107\n","Epoch [100/100], Step [1/12], Loss: 0.8378\n","Epoch [100/100], Step [2/12], Loss: 0.7810\n","Epoch [100/100], Step [3/12], Loss: 0.7442\n","Epoch [100/100], Step [4/12], Loss: 0.8049\n","Epoch [100/100], Step [5/12], Loss: 0.6984\n","Epoch [100/100], Step [6/12], Loss: 0.7404\n","Epoch [100/100], Step [7/12], Loss: 0.8758\n","Epoch [100/100], Step [8/12], Loss: 0.8065\n","Epoch [100/100], Step [9/12], Loss: 0.8285\n","Epoch [100/100], Step [10/12], Loss: 0.8316\n","Epoch [100/100], Step [11/12], Loss: 0.8194\n","Epoch [100/100], Step [12/12], Loss: 0.7107\n"],"name":"stdout"}]},{"metadata":{"id":"A_EoMcLtgGkK","colab_type":"text"},"cell_type":"markdown","source":["# Test model"]},{"metadata":{"id":"iS2I2oxbgFzq","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","#with torch.no_grad():\n","#     correct = 0\n","#     total = 0\n","#     for images, labels in test_loader:\n","#         images = images.reshape(-1, 28*28).to(device)\n","#         labels = labels.to(device)\n","#         outputs = model(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","\n","#     print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IE_054BKg2yu","colab_type":"text"},"cell_type":"markdown","source":["# Save the NN weights"]},{"metadata":{"id":"q0XDbM2fYZLL","colab_type":"code","colab":{}},"cell_type":"code","source":["# # Save the model checkpoint\n","# torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":0,"outputs":[]}]}